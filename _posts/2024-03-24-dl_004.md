---
layout: single
title:  "딥러닝) RAG(Retrieval Augmented Generation)"
categories: deeplearning
tag: [python, deeplearning, tensorflow, RAG]
toc: true
author_profile: false
use_math: true
---

[4월 이후 수정예정]
- 팀 프로젝트에 RAG를 이용하기로 결정하여 기본적인 지식을 습득한 뒤, 논문은 개인적으로 조금 씩 읽어볼 예정이다.

## 1. RAG 원문 링크
- [RAG 논문 보기](https://arxiv.org/abs/2005.11401)

## 2. RAG의 요점
- 정의 : 검색기를 활용하여 관련된 컨텍스트를 찾아, LLM에게 제공함으로써, 환각증상(Halluciation)을 해결하고 사용자가 요구하는 대답을 더욱 정확히 할 수 있도록 하는 프롬프트 엔지니어의 일종이라고 보는 것이 좋을 듯하다.

- 도구
    1. 로더 : 텍스트 추출 / 일반적으로 PyPDF 등과 같은 PDF리더기 등이 사용되어 텍스트를 추출함
    2. 토크나이저 : 단어 번호 매기기 / 입력된 텍스트를 형태소로 분리하여 각 형태소에 번호를 부여
    3. 임베딩기 : 위치 배정 / 단어를 다차원 공간에 분포시켜서 거리 측정을 통해 유사도를 도출하는 도구
        ![임베딩기](https://d2908q01vomqb2.cloudfront.net/77de68daecd823babbb58edb1c8e14d7106e83bb/2023/08/02/WhyLabs-ML-Embeddings-1.png)
    4. VectorDB : 검색할 텍스트를 보관한 DB / sqlite3 등
    5. retriver : 검색기 / 텍스트를 검색하는 것
    6. LLM : 거대 언어 모델 / gemini 등

## 3. 코드 예시
- 참조 : [https://www.youtube.com/watch?v=Xp7yFtpmXPk](https://www.youtube.com/watch?v=Xp7yFtpmXPk)

    ```python
    ## 관련 라이브러리 다운로드
    ! pip install -U --quiet langchain-google-genai
    ! pip install -U --quiet langchain tiktoken pypdf sentence_transformers chromadb

    from langchain.document_loaders import PyPDFLoader
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    from langchain.embeddings import HuggingFaceEmbeddings
    from langchain.vectorstores import Chroma
    from langchain_google_genai import ChatGoogleGenerativeAI
    from langchain.prompts import ChatPromptTemplate
    from langchain.schema.runnable import RunnableMap
    import os

    ## 1. 로더(PDF시 PDF리더, TEXT시 단순 텍스트)
    ### 로더는 종류별로 각종 메타데이터(텍스트 이외의 정보 ex)페이지 번호 등)
    loader = PyPDFLoader("읽을 텍스트.pdf")
    pages = loader.load_and_split() ## 페이지별로 찢기

    ## 2. 토크나이저
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50) 
    texts = text_splitter.split_documents(pages)

    # 3. 임베딩기 선정(Open ai 또는 Hugging face)
    model_name = "jhgan/ko-sbert-nli" # 3. 임베딩기
    model_kwargs = {'device': 'cpu'}
    encode_kwargs = {'normalize_embeddings': True}
    hf = HuggingFaceEmbeddings(
        model_name=model_name,
        model_kwargs=model_kwargs,
        encode_kwargs=encode_kwargs
    )

    ## 4. VectorDB(Chroma)
    docsearch = Chroma.from_documents(texts, hf) 

    ## 5. 검색기
    retriever = docsearch.as_retriever(
                                    search_type="mmr",
                                    search_kwargs={'k':5, 'fetch_k': 50}) ## k는 검색 유사도 문장 수
    retriever.get_relevant_documents("검색 키워드") ## 이 후 chain.invoke question이 들어올 곳


    ## 프롬프트
    template = """Answer the question as based only on the following context:
    {context}

    Question: {question}
    """

    prompt = ChatPromptTemplate.from_template(template)

    ## 6. LLM 선정
    os.environ['GOOGLE_API_KEY'] = "gemini-api-key"
    gemini = ChatGoogleGenerativeAI(model="gemini-pro", temperature = 0)

    ## RAG
    chain = RunnableMap({
        "context": lambda x: retriever.get_relevant_documents(x['question']),
        "question": lambda x: x['question']
    }) | prompt | gemini


    Markdown(chain.invoke({'question': "묻고 싶은 문장"}).content)

    ```

<br/>

- 텍스트로 읽을 시

    ```python
    with open('읽힐 텍스트.txt', 'r') as file:
        lines = file.readlines()

    docsearch = Chroma.from_texts(lines, hf)
    ```


