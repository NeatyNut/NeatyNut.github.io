---
layout: single
title:  "딥러닝) RAG(Retrieval Augmented Generation)"
categories: deeplearning
tag: [python, deeplearning, tensorflow, RAG]
toc: true
author_profile: false
use_math: true
---
~ 논문 읽는 중

## 1. RAG 원문 링크
- [RAG 논문 보기](https://arxiv.org/abs/2005.11401)

## 2. RAG의 요점
- 정의 : 검색기를 활용하여 관련된 컨텍스트를 찾아, LLM에게 제공함으로써, 환각증상(Halluciation)을 해결하고 사용자가 요구하는 대답을 더욱 정확히 할 수 있도록 하는 프롬프트 엔지니어의 일종이라고 보는 것이 좋을 듯하다.

- 도구
    1. 로더 : 텍스트 추출 / 일반적으로 PyPDF 등과 같은 PDF리더기 등이 사용되어 텍스트를 추출함
    2. 텍스트 스플리터 : 긴 문서를 작은 단위인 청크(chunk)로 나누는 텍스트 분리 도구 → 전반적인 콘텍스트 맥락 파악을 강화하기 위해 일부 문장 중첩 가능
    3. 임베딩기 : 위치 배정 / 단어를 다차원 공간에 분포시켜서 거리 측정을 통해 유사도를 도출하는 도구(그림출처 : AWS)
        ![임베딩기](https://d2908q01vomqb2.cloudfront.net/77de68daecd823babbb58edb1c8e14d7106e83bb/2023/08/02/WhyLabs-ML-Embeddings-1.png)
    4. VectorDB : 검색할 텍스트를 보관한 DB / sqlite3 등
    5. retriver : 검색기 / 텍스트를 검색하는 것
    6. LLM : 거대 언어 모델 / gemini 등

## 3. 코드 예시
- 참조 : [https://www.youtube.com/watch?v=Xp7yFtpmXPk](https://www.youtube.com/watch?v=Xp7yFtpmXPk)

    ```python
    ## 관련 라이브러리 다운로드
    ! pip install -U --quiet langchain-google-genai
    ! pip install -U --quiet langchain tiktoken pypdf sentence_transformers chromadb

    from langchain.document_loaders import PyPDFLoader
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    from langchain.embeddings import HuggingFaceEmbeddings
    from langchain.vectorstores import Chroma
    from langchain_google_genai import ChatGoogleGenerativeAI
    from langchain.prompts import ChatPromptTemplate
    from langchain.schema.runnable import RunnableMap
    import os

    ## 1. 로더(PDF시 PDF리더, TEXT시 단순 텍스트)
    ### 로더는 종류별로 각종 메타데이터(텍스트 이외의 정보 ex)페이지 번호 등)
    loader = PyPDFLoader("읽을 텍스트.pdf")
    pages = loader.load_and_split() ## 페이지별로 찢기

    ## 2. 토크나이저
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50) 
    texts = text_splitter.split_documents(pages)

    # 3. 임베딩기 선정(Open ai 또는 Hugging face)
    model_name = "jhgan/ko-sbert-nli" # 3. 임베딩기
    model_kwargs = {'device': 'cpu'}
    encode_kwargs = {'normalize_embeddings': True}
    hf = HuggingFaceEmbeddings(
        model_name=model_name,
        model_kwargs=model_kwargs,
        encode_kwargs=encode_kwargs
    )

    ## 4. VectorDB(Chroma)
    docsearch = Chroma.from_documents(texts, hf) 

    ## 5. 검색기
    retriever = docsearch.as_retriever(
                                    search_type="mmr",
                                    search_kwargs={'k':5, 'fetch_k': 50}) ## k는 검색 유사도 문장 수
    retriever.get_relevant_documents("검색 키워드") ## 이 후 chain.invoke question이 들어올 곳


    ## 프롬프트
    template = """Answer the question as based only on the following context:
    {context}

    Question: {question}
    """

    prompt = ChatPromptTemplate.from_template(template)

    ## 6. LLM 선정
    os.environ['GOOGLE_API_KEY'] = "gemini-api-key"
    gemini = ChatGoogleGenerativeAI(model="gemini-pro", temperature = 0)

    ## RAG
    chain = RunnableMap({
        "context": lambda x: retriever.get_relevant_documents(x['question']),
        "question": lambda x: x['question']
    }) | prompt | gemini


    Markdown(chain.invoke({'question': "묻고 싶은 문장"}).content)

    ```

<br/>

- 텍스트로 읽을 시

    ```python
    with open('읽힐 텍스트.txt', 'r') as file:
        lines = file.readlines()

    docsearch = Chroma.from_texts(lines, hf)
    ```

## 4. 논문 리뷰
### Abstract
- RAG기법의 등장배경
    - 요약 : LLM의 지식 기반 작업에 대한 능력, 자신의 결정에 대한 근거를 제시하거나 지식을 업데이트하는 것은 여전히 제한적
    - 원문 : their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems
- RAG 소개
    - 요약 : 사전 학습된 시퀀스 모델과 위키피디아 벡터와 검색기가 결합된 RAG모델을 소개
    - 원문 : We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever
- 실험 내용
    - 요약 : 두가지 RAG 기법을 비교(생성된 시퀀스 전반에 같은 구절 vs 토큰당 다른 구절)
    - 원문 : We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token

### Introduction
- RAG 소개
    - 요약 : RAG는 위키피디아 벡터를 사전 학습된 뉴럴 검색기 통해 접근하는 사전 학습된 시퀀스 트랜스포머 모델
    - 원문 : We endow pre-trained, parametric-memory generation models with a non-parametric memory through a general-purpose fine-tuning approach which we refer to as retrieval-augmented generation (RAG). We build RAG models where the parametric memory is a pre-trained seq2seq transformer, and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever
- RAG의 강점
    - 요약(의역) : 학습된(parametric) 지식, 학습되지 않은(non-parametric) 지식의 결합으로 인간조차 외부 지식 원천이 필요한 분야인 지식 기반 작업을 가능케함
    - 원문 : Our results highlight the benefits of combining parametric and non-parametric memory with generation for nowledge-intensive tasks—tasks that humans could not reasonably be expected to perform without access to an external knowledge source.
- 주요 용어
    - DPR(Dense Passage Retriever) : 검색기
    - BART(seq2seq model) : 시퀀스 모델

### Methods
- 주요 용어
    - $x$ : the input sequence
    - $y$ : target sequence
    - $z$ : a retrieved passage
    - $p_{\eta}(z\|x)$ : a retriever with parameters $\eta$
    - $p_{\theta}(y_{i}\|x,z,y_{1:i-1})$ : a generator
        - $i$ : time
        - $\theta$ : a current token based on a context of the previous $i-1$ tokens $y_{1:i-1}$
- RAG-Sequence Model
    - 요약 : 시퀀스 확률(p(y$\|$x))를 얻기 위해 검색된 문서를 하나의 숨겨진 변수로 활용
    - 원문 : Technically, it treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y$\|$x) via a top-K approximation
    - 수식
        - $P_{RAG-Sequence}(y\|x) \approx \Sigma_{z \in top-k(p(\cdot\|x))}_{p_{\eta}(z\|x)} \Pi_{z \in top-k(p(\cdot\|x))}_{p_{\theta}(y_{i}\|x,z,y_{1:i-1})}$

