---
layout: single
title:  "논문 정리) Deep Neural Networks for YouTube Recommendations"
categories: deeplearning
tag: [python, deeplearning, tensorflow, Youtube Algorithm, recommendation]
toc: true
author_profile: false
use_math: true
---


## 1. 원문 링크
- [논문 보기](https://static.googleusercontent.com/media/research.google.com/ko//pubs/archive/45530.pdf)

## 2. 논문 리뷰
### Abstract
- 첫 번째로, 후보 생성(candidates generation)을 위한 딥러닝 모델에 대해 설명하고, 그 후에 별도의 딥러닝 기반 순위 매기기(ranking) 모델에 대해 다룹니다.
	- 원문 : The paper is split according to the classic two-stage information retrieval dichotomy: first, we detail a deep candidate generation model and then describe a separate deep ranking model

### 1. Introduction
- 이 문서에서는 딥러닝이 최근 Youtube 동영상 추천 시스템에 미친 엄청난 영향에 중점을 둘 것입니다.
	- 원문 : In this paper we will focus on the immense impact deep learning has recently had on the YouTube video recommendations system.

- 동영상 추천하는 것은 세 가지 관점에서 어렵습니다.
	- 원문 : recommending YouTube videos is extremely challenging from three major perspectives

- Scale : 기존의 많은 추천 알고리즘은 작은 문제엔 잘 작동하나 우리의 규모엔 실패하였습니다.
	- 원문 :  Scale: Many existing recommendation algorithms proven to work well on small problems fail to operate on our scale.

- Freshness : 새로 업로된 컨텐츠에 대해서도 잘 반응하는 모델이어야 한다.
	- 원문 : The recommendation system should be responsive enough to model newly uploaded content as well as the latest actions taken by the user

- Noise : 사용자 만족도를 알기 어려워 간접적 피드백으로 확인합니다.
	- 원문 : Historical user behavior on YouTube is inherently difficult to predict due to sparsity and a variety of unobservable external factors. We rarely obtain the ground truth of user satisfaction and instead model noisy implicit feedback signals

### 2. System overview
![image1](https://lh3.googleusercontent.com/KFhbf9GNaCfC68pvp9eUKaGoDD-vtLlNoTOfumJcY9ZIz4UbGi20t6x5LAGL0qOGH-ZZqro40jxv4D-fXOIBsLqWh8HLjxKW7L7FSWaS9EmWSe2hAyDenspjDc6Il8HGMnF1Fl4XSyhFXu5yB1hT4yk)

- 두개의 네트워크로 구성됨 : 후보자 생성, 랭킹
	- 원문 : The system is comprised of two neural networks: one for candidate generation and one for ranking

#### 2.1 candidate generation
- 입력으로 User history를 이용하여 몇 백개의 후보 검색
	- 원문 : The candidate generation network takes events from the user’s YouTube activity history as input and retrieves a small subset (hundreds) of videos from a large corpus

- 정밀도를 높이는 것이 중요
	- 원문 : These candidates are intended to be generally relevant to the user with high precision.

#### 2.2 ranking
- 높은 재현율을 가진 후보들 중 최고를 가리기 위해선 fine-level representation이 필요
	- 원문 : Presenting a few “best” recommendations in a list requires a fine-level representation to distinguish relative importance among candidates with high recall

### 3. Candidate generation
#### 3.1 Recommendation as Classification
<center>
$$
P(w_t = i|U, C) = \frac{e^{v_i u}}{\sum_{j \in V} e^{v_j u}}
$$
</center>

- 이 예측 문제는 사용자 U와 컨텍스트 C에 기반하여 수백만 개의 비디오 i(클래스) 중 특정 시점 t에서 비디오 wt를 정확히 분류하는 문제로 귀결

- 상위 N개의 비디오를 선택하기 위해 가능성 높은 N개의 클래스를 계산해야 하며, 수백만 개의 항목을 짧은 시간 내에 처리하기 위해서 서브선형 방식의 대략적 점수 산정 방식을 사용함.
	- 원문 : At serving time we need to compute the most likely N classes (videos) in order to choose the top N to present to the user. Scoring millions of items under a strict serving latency of tens of milliseconds requires an approximate scoring scheme sublinear in the number of classes.

#### 3.2 Model Architecture
- 임베딩은 역전파 업데이트를 통해 다른 모든 모델 매개변수와 공동으로 학습됨.
	- 원문 : the embeddings are learned jointly with all other model parameters through normal gradient descent backpropagation updates

#### 3.3 Heterogeneous Signals
- 딥 뉴럴 네트워크를 행렬 분해의 일반화로 사용하는 주요 이점은 임의의 연속적 및 범주형 특징을 모델에 쉽게 추가할 수 있다는 점
	- 원문 : A key advantage of using deep neural networks as a generalization of matrix factorization is that arbitrary continuous and categorical features can be easily added to the model.
- 머신러닝 시스템은 종종 과거에 기반하기에 편향적인 예측을 내놓을 수 있다.
	- 원문 : Machine learning systems often exhibit an implicit bias towards the past because they are trained to predict future behavior from historical examples
- 비디오 인기도의 변화에 대응하기 위해 훈련 예시의 "나이"를 특징으로 추가하고, 예측 시에는 이를 0으로 설정
	- 원문 : we feed the age of the training example as a feature during training. At serving time, this feature is set to zero (or slightly negative) to reflect that the model is making predictions at the very end of the training window.

![image2](https://lh3.googleusercontent.com/gPfDRyK7mVq0IgUb-Wy_r0lGRNpd8pgJC7DGJSzL5m9-k4qmnnNcGqPMQuc5ZAYhiolMn3JpCSg1FsubLCX4XDwIS6cZ4oOcKU1VsGTJmC287ku3THbNLeio8R-WypGYQbvX6pxin452GrvhxnJP8sM)

#### 3.4 Label and Context Selection
- 대리 학습 문제의 선택이 A/B 테스트 성능에 매우 큰 영향을 미친다는 것을 발견했지만, 오프라인 실험으로는 측정하기가 매우 어렵다는 것
	- 원문 : We have found that the choice of this surrogate learning problem has an outsized importance on performance in A/B testing but is very difficult to measure with offline experiments.
- 사용자당 고정된 수의 학습 예제를 생성하여 소수의 활발한 사용자가 손실을 지배하지 못하도록 하고, 모든 사용자가 균등하게 반영되도록 했습니다.
	- 원문 : Another key insight that improved live metrics was to generate a fixed number of training examples per user, effectively weighting our users equally in the loss function. This prevented a small cohort of highly active users from dominating the loss.
![image3](https://lh3.googleusercontent.com/1fpR_oW2xdjr_sLDQ54YPgUXX72bqxrYxLcz9YIpUT06BcfRJZgTeBF_wLF1h9_8xMBze_SdS646IuniY7rxS1CzMtOuI7NCin2H76LrJS1NKrSzHdmpHezrhkc1mSQ3pjmFqC2_dZ_pKEErFrwAqa4)
- 가변 크기의 희소 ID 집합을 고정된 크기의 벡터로 변환하기 위해 임베딩을 평균화하여 연결
	- 원문 : Embeddings are averaged before concatenation to transform variable sized bags of sparse IDs into fixed-width vectors suitable for input to the hidden layers.

#### 3.5 Experiments with Features and Depth
- 피쳐와 레이어를 추가하는 것은 성능 향상에 도움을 줌
	- 원문 : Adding features and depth significantly improves precision on holdout data as shown in Figure 6
![image4](https://lh5.googleusercontent.com/MWYKBk0Q_jjkkKTa3cSqPtzIlYNoz1mLehPltuhrUuFpp2DeNL38XCGP3tlTUEY8zDZqixMKjpKOK7Fv1eGzj5HsP_E1H5h2zFkDtZx_LNz1bulLrg5ltMGCtp-LnlfCUyz1vjsWJn11p8ktfbEkT70)


- 모델에 대한 레이블과 입력 컨텍스트를 선택하는 것은 오프라인에서 평가하기 어렵지만, 라이브 성능에 큰 영향을 미침
	- 원문 : Choosing labels and input context to the model is challenging to evaluate offline but has a large impact on live performance.
![image5](https://lh3.googleusercontent.com/yXXCMuxocWD4Fp8YhrnsfuPybnmnADDHKRp70pj5t0zkX3gJabwgexkE47Ax6oSpYYU8gnjtFAy_Rh1bMO4tcKw24aODwFr10Ll7rnsYjbnE8jFzIX5N7POLTt7ELRf0VKQCFLqk2G3FJOs3TumuEFQ)

### 4. RANKING
- 랭킹 단계에서는 소수의 동영상만 평가하기 때문에 동영상과 사용자 관계를 설명하는 더 많은 특징을 활용할 수 있다.
	- 원문 : During ranking, we have access to many more features describing the video and the user’s relationship to the video because only a few hundred videos are being scored rather than the millions scored in candidate generation

#### 4.1 Feature Representation
- 사용자 행동의 시간적 순서를 표현하고 그것이 동영상 노출과의 관계를 나타내는 것이 핵심 과제입니다. 우리가 관찰한 가장 중요한 신호는 사용자의 이전 상호작용을 해당 항목 자체와 관련하여 설명하는 신호입니다.
	- 원문 : The main challenge is in representing a temporal sequence of user actions and how these actions relate to the video impression being scored. We observe that the most important signals are those that describe a user’s previous interaction with the item itself

- 만약 사용자가 최근에 추천된 동영상을 시청하지 않았다면, 모델은 자연스럽게 다음 페이지 로드 시 이 노출을 낮춥니다.
	- 원문 : If a user was recently recommended a video but did not watch it then the model will naturally demote this impression on the next page load.

- 동일한 ID 공간의 범주형 특징들은 임베딩을 공유하지만, 각 특징은 개별적으로 네트워크에 입력되어 상위 레이어에서 각 특징에 대한 특화된 표현을 학습합니다.
	- 원문 : Importantly, categorical features in the same ID space also share underlying emeddings. For example, there exists a single global embedding of video IDs that many distinct features use (video ID of the impression, last video ID watched by the user, video ID that “seeded” the recommendation, etc.). Despite the shared embedding, each feature is fed separately into the network so that the layers above can learn specialized representations per feature

- 적절한 정규화는 수렴하는데 필수적이다.
	- 원문 : We found that proper normalization of continuous features was critical for convergence.

![image6](https://lh6.googleusercontent.com/Rsz2_PzmrqmgtXBKjc60lkxC8pADBbcW_Uq59kjsjRhSvrJlBKx--S8aWbh5qYnW5lrmX8v12DEOXcbOjEAUHY9nSfAbM4N8GN2WSDyIQLMUSBFRHkSxFDLVqfuQa7nJxRdF9liyucLE7iCUTRVCX9s)

#### 4.2 Modeling Expected Watch Time
- 시청시간을 예측하기 위해 가중치로된 로지스틱 회귀 기술(loss function : cross-entropy)을 사용
	- 원문 : To predict expected watch time we use the technique of weighted logistic regression, which was developed for this purpose. The model is trained with logistic regression under crossentropy loss


<center>$$The\ odds\ learned = \frac {\sum T_i}{N-k} \approx E[T](1+P)$$</center>
> $N$ = 학습 데이터 수<br/>
> $k$ = positive 데이터<br/>
> $T_i$ = i번째 영상물 시청시간<br/>
> $E[T]$ = 예측된 시청시간<br/>
> $P$ = 클릭 확률(P가 엄청 작기에 $E[T]$로 수렴)

#### 4.3 Experiments with Hidden Layers
- 두 노출을 점수화한 후, 긍정적인 노출의 시청 시간이 잘못 예측되었다면, 이 시청 시간을 사용자별로 가중하여 총 시청 시간 대비 잘못 예측된 시간의 비율로 손실을 계산합니다.
	- 원문 : We first score these two impressions with our model. If the negative impression receives a higher score than the positive impression, then we consider the positive impression’s watch time to be mispredicted watch time. Weighted, peruser loss is then the total amount mispredicted watch time as a fraction of total watch time over heldout impression pairs.

- 정규화된 연속형 특징만 사용하면 손실이 0.2% 증가했고, 긍정적 및 부정적 예제를 동일한 가중치로 훈련하면 시청 시간 가중 손실이 4.1% 크게 증가했습니다.
	- 원문 : For the 1024 → 512 → 256 model we tried only feeding the normalized continuous features without their powers, which increased loss by 0.2%. With the same hidden layer configuration, we also trained a model where positive and negative examples are weighted equally. Unsurprisingly, this increased the watch time-weighted loss by a dramatic 4.1%.

![image7](https://lh3.googleusercontent.com/ywW2xUdzQPirUv_TMqXdYPlBCdapZO6-lFCIRHJPGQfJcluQixfDEQmsJOG0NPjIlF37i7NI6VvvSkWYSHk5MlSNZ1Sw_VTKgHLQxeEdlngKSZlyNB--YumwJBCitDkd_StLf8jblIuEEP7eRoEoe_w)

### 5. CONCLUSIONS
- 우리는 YouTube 비디오 추천을 위해 심층 신경망 아키텍처를 사용하며, 이를 후보 생성과 순위화로 나누어 문제를 해결했습니다. 이 모델은 다양한 신호를 효과적으로 통합하고 깊은 레이어를 통해 상호작용을 잘 모델링하여, 기존의 매트릭스 분해 방법보다 성능이 뛰어납니다.
	- 원문 : We have described our deep neural network architecture for recommending YouTube videos, split into two distinct problems: candidate generation and ranking. Our deep collaborative filtering model is able to effectively assimilate many signals and model their interaction with layers of depth, outperforming previous matrix factorization approaches used at YouTube
